{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Our models code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### installing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZDP_KnrqLFH",
        "outputId": "d5ee207e-16c1-43da-8735-c81b57fb856a"
      },
      "outputs": [],
      "source": [
        "!pip install deepface retina-face face_recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### importing these libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0WgUIHhqLFN"
      },
      "outputs": [],
      "source": [
        "from deepface import DeepFace\n",
        "from retinaface import RetinaFace\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import face_recognition\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### helper methods used later on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WXy6Q3jiqLFO"
      },
      "outputs": [],
      "source": [
        "def age_range(age):\n",
        "    if age < 0 or age > 100:\n",
        "        return \"Out of Range\"\n",
        "    else:\n",
        "        return f\"{int(age // 10) * 10}-{int(age // 10) * 10 + 10}\"\n",
        "\n",
        "def crop(frame, x1, y1, x2, y2):\n",
        "    cropped = frame[int(y1/1.2):int(y2*1.2), int(x1/1.2):int(x2*1.2)]\n",
        "    return cropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### classifying video path that we want to analyze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### if we want to used a surveillance camera for real time use change video capture to specified camera ex. 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkyg81nAqLFS"
      },
      "outputs": [],
      "source": [
        "video_path = \"video path.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### creating fucntions for face recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleFacerec:\n",
        "    def __init__(self):\n",
        "        self.known_face_encodings = []\n",
        "        self.known_face_names = []\n",
        "        self.frame_resizing = 0.25\n",
        "\n",
        "    def load_encoding_images(self, images_path):\n",
        "        images_path = glob.glob(os.path.join(images_path, \"*.*\"))\n",
        "        print(\"{} encoding images found.\".format(len(images_path)))\n",
        "        for img_path in images_path:\n",
        "            img = cv2.imread(img_path)\n",
        "            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            basename = os.path.basename(img_path)\n",
        "            (filename, ext) = os.path.splitext(basename)\n",
        "            img_encoding = face_recognition.face_encodings(rgb_img)[0]\n",
        "            self.known_face_encodings.append(img_encoding)\n",
        "            self.known_face_names.append(filename)\n",
        "        print(\"Encoding images loaded\")\n",
        "\n",
        "    def detect_known_faces(self, frame):\n",
        "        small_frame = cv2.resize(frame, (0, 0), fx=self.frame_resizing, fy=self.frame_resizing)\n",
        "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
        "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
        "\n",
        "        face_names = []\n",
        "        for face_encoding in face_encodings:\n",
        "            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
        "            name = \"Unknown\"\n",
        "            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
        "            best_match_index = np.argmin(face_distances)\n",
        "            if matches[best_match_index]:\n",
        "                name = self.known_face_names[best_match_index]\n",
        "            face_names.append(name)\n",
        "\n",
        "        face_locations = np.array(face_locations)\n",
        "        face_locations = face_locations / self.frame_resizing\n",
        "        return face_locations.astype(int), face_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### here is main cell for running the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### after done processing it is saved as seperate video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### graph is plotted after done showing number of faces detected in each frame "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPKNnjwFqLFT",
        "outputId": "46aedbaa-2f22-4988-b650-9e143ddc48d5"
      },
      "outputs": [],
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out_video = cv2.VideoWriter(\"output.mp4\", fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "frames_processed = 1\n",
        "sfr = SimpleFacerec()\n",
        "sfr.load_encoding_images(\"images/\")\n",
        "face_counts_graph = []\n",
        "while (frames_processed != length):\n",
        "    ret, frame = cap.read()\n",
        "    print(f\"Processing frame {frames_processed}/{length}\")\n",
        "    if not ret or cv2.waitKey(1) == ord('q'):\n",
        "      print(\"well that's bad\")\n",
        "      break\n",
        "    faces = RetinaFace.detect_faces(frame, threshold = 0.8)\n",
        "    face_count = len(faces)\n",
        "    face_counts_graph.append(face_count)\n",
        "    count = 0\n",
        "    for face in faces:\n",
        "        x1,y1,x2,y2 = faces[face]['facial_area']\n",
        "        cropped = crop(frame,x1,y1,x2,y2)\n",
        "        face_locations, face_names = sfr.detect_known_faces(cropped)\n",
        "        if(len(face_names)>0):\n",
        "          found = face_names[0]\n",
        "        else:\n",
        "          found = \"unknown\"\n",
        "        try:\n",
        "          AGR = DeepFace.analyze(cropped, actions=['age', 'gender', 'race'])\n",
        "          if(AGR[0]['face_confidence']>0.8):\n",
        "            print(AGR[0]['face_confidence'])\n",
        "            text1 = f\"Conf:{AGR[0]['face_confidence']}-Age:{age_range(AGR[0]['age'])}\"\n",
        "            text2 = f\"Gender:{AGR[0]['dominant_gender']}-race:{AGR[0]['dominant_race']}\"\n",
        "            cv2.rectangle(frame, (x1,y1), (x2, y2), (255, 0, 0), 4)\n",
        "            cv2.putText(frame, text1,(x1, y1 - 26), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 0, 0),1)\n",
        "            cv2.putText(frame, text2,(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 0, 0),1)\n",
        "            cv2.putText(frame, found,(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 0, 0),1)\n",
        "        except:\n",
        "            cv2.rectangle(frame, (x1,y1), (x2, y2), (0, 0, 255), 4)\n",
        "            cv2.putText(frame, f\"Confidence:{round(faces[face]['score']*100,-4)}\",(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255),1)\n",
        "            cv2.putText(frame, found,(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255),1)\n",
        "    frames_processed += 1\n",
        "    out_video.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out_video.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Video processing complete. Output video saved as 'output.mp4'.\")\n",
        "\n",
        "plt.plot(range(len(face_counts_graph)), face_counts_graph) \n",
        "\n",
        "plt.xlabel(\"Frame Number\")\n",
        "plt.ylabel(\"Number of Faces Detected\")\n",
        "plt.title(\"Number of Faces Detected per Frame\")\n",
        "\n",
        "with open(\"face_counts.txt\", \"w\") as f:\n",
        "  f.write(\"[\")\n",
        "  for count in face_counts_graph:\n",
        "    f.write(str(count) + \",\")\n",
        "  f.write(\"]\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
